{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7d15c",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5122654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from starvers.starvers import TripleStoreEngine\n",
    "import seaborn as sns\n",
    "from scipy.io import arff\n",
    "import json\n",
    "from rdflib import Graph, Namespace, URIRef, Literal, BNode\n",
    "from rdflib.namespace import RDF, RDFS, XSD, DCTERMS\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a42ca9",
   "metadata": {},
   "source": [
    "# Task Create a machine-actionable description of the dataset \n",
    "- [ ] following Croissant / Schema.org descriptions for datasets \n",
    "  - [ ] Croissant: https://neurips.cc/virtual/2024/poster/97627, https://docs.mlcommons.org/croissant/docs/croissant-spec.html; \n",
    "  - [ ] schema.org: https://schema.org/Dataset, \n",
    "  - [ ] Example:\n",
    "  - [ ] JSON example provided at https://schema.org/Dataset#eg-0478)\n",
    "  \n",
    "Note: Task already done, as we have a dataset that is already in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the Data Frame\n",
    "\n",
    "dataset_path = \"\" # File has to be in the same directory as the notebook\n",
    "file_name = 'php0FyS2T.arff' # Filename in .arff format \n",
    "\n",
    "def load_arff_data() -> pd.DataFrame:\n",
    "    \n",
    "    input_file = os.path.join(dataset_path, file_name) \n",
    "    \n",
    "    # Use scipy's arff loader to handle the @attribute metadata and @data sections\n",
    "    raw_data, meta = arff.loadarff(input_file)\n",
    "    \n",
    "    # Convert the raw structured array to a pandas DataFrame\n",
    "    dataframe = pd.DataFrame(raw_data)\n",
    "\n",
    "    def clean_data(df: pd.DataFrame):\n",
    "        # ARFF loaders often read nominal/string attributes as bytes (e.g., b'1').\n",
    "        # This function decodes them back to standard strings or integers.\n",
    "        if 'Class' in df.columns and df['Class'].dtype == object: # Convert original string values (in case they were wrongly importated as \"byte\") back to to string or integer\n",
    "             df['Class'] = df['Class'].str.decode('utf-8').astype(int)\n",
    "        return df\n",
    "\n",
    "    loaded_data = dataframe\n",
    "    loaded_data = clean_data(loaded_data)\n",
    "    \n",
    "    return loaded_data\n",
    "\n",
    "# Execute\n",
    "df = load_arff_data()\n",
    "\n",
    "display(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee5b1d",
   "metadata": {},
   "source": [
    "# Task: Analyze and describe the characteristics of the dataset\n",
    "\n",
    "- [ ] Analyze and describe the characteristics of the dataset \n",
    "  - [ ] size, \n",
    "  - [ ] attribute types as discussed in class, \n",
    "  - [ ] value ranges, \n",
    "  - [ ] sparsity, \n",
    "  - [ ] min/max values, \n",
    "  - [ ] outliers, \n",
    "  - [ ] missing values, \n",
    "  - [ ] correlations, ...\n",
    "- [ ] **Provenance Graph:** Describe aa this in the provenance graph. \n",
    "- [ ] **RDF:** Also, describe any hypotheses you might have concerning the \n",
    "  - [ ] distribution of the data, \n",
    "  - [ ] number of clusters and their relationship, \n",
    "  - [ ] majority/minority classes as rdf comment field in the provenance graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis\n",
    "\n",
    "def analyze_dataset(df: pd.DataFrame):\n",
    "    print(\"=== Dataset Overview ===\")\n",
    "    n_instances, n_attributes = df.shape\n",
    "    print(f\"Number of instances: {n_instances}\")\n",
    "    print(f\"Number of attributes: {n_attributes}\")\n",
    "    \n",
    "    # Check for Class attribute\n",
    "    if 'Class' not in df.columns:\n",
    "        target_col = df.columns[-1]  # Sometimes the target is the last column with a different name\n",
    "        print(f\"Target column assumed to be: '{target_col}'\")\n",
    "    else:\n",
    "        target_col = 'Class'\n",
    "        \n",
    "    print(f\"\\n=== Attribute Types ===\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    print(\"\\n=== Missing Values ===\")\n",
    "    missing_counts = df.isnull().sum()\n",
    "    total_missing = missing_counts.sum()\n",
    "    print(f\"Total missing values: {total_missing}\")\n",
    "    if total_missing > 0:\n",
    "        print(missing_counts[missing_counts > 0])\n",
    "        \n",
    "    print(\"\\n=== Value Ranges & Statistics ===\")\n",
    "    # numeric_df excludes the target if it's categorical\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    stats = numeric_df.describe().T\n",
    "    stats['range'] = stats['max'] - stats['min']\n",
    "    display(stats[['min', 'max', 'mean', 'std', 'range']].head())\n",
    "\n",
    "    print(\"\\n=== Sparsity ===\")\n",
    "    zero_counts = (numeric_df == 0).sum().sum()\n",
    "    total_cells = numeric_df.size\n",
    "    sparsity = zero_counts / total_cells\n",
    "    print(f\"Sparsity (percentage of zeros): {sparsity:.2%}\")\n",
    "\n",
    "    print(\"\\n=== Class Distribution (Majority/Minority) ===\")\n",
    "    class_counts = df[target_col].value_counts()\n",
    "    print(f\"Number of classes: {len(class_counts)}\")\n",
    "    print(f\"Min class size: {class_counts.min()}\")\n",
    "    print(f\"Max class size: {class_counts.max()}\")\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 1. Class Distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # Using a subset if too many classes\n",
    "    if len(class_counts) > 20:\n",
    "        sns.histplot(class_counts, bins=10, kde=False)\n",
    "        plt.title('Histogram of Class Sizes')\n",
    "        plt.xlabel('Number of Instances per Class')\n",
    "    else:\n",
    "        sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "        plt.title('Class Distribution')\n",
    "        \n",
    "    # 2. Correlation Matrix (features only)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Subsampling features if too many for a clean plot\n",
    "    corr_matrix = numeric_df.iloc[:, :20].corr() \n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', annot=False)\n",
    "    plt.title('Correlation Matrix (First 20 Features)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return stats, class_counts\n",
    "\n",
    "# Execute Analysis\n",
    "stats, class_counts = analyze_dataset(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c1ded0",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "1) Train a reasonably sized „regular“ SOM \n",
    "- Train a SOM with „regular“ size (i.e. number of units as a certain fraction of the number of data items) and reasonable training parameters (sufficiently large initial neighborhood, learning rate; provide a justification for the selection of the parameters. \n",
    "- NOTE: Learning rates for SOMs differ from those usually encountered in Deep Neural Networks, c.f. lecture)\n",
    "- Analyse in detail the class distribution, cluster structure, quantization errors, topology violations. \n",
    "    - a) Can you identify the border effect and magnification factors. \n",
    "    - b) How well do class distribution and cluster structure match? \n",
    "    - c) Which classes fall into sub-clusters, which classes are split across clusters, which classes mix in clusters. \n",
    "    - d) How is the quantization error distributed on the map, how does this correspond with perceived cluster separation and quality?\n",
    "- Describe and compare the structures found (providing detailed info on visualizations and parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 (Gemini Version)\n",
    "\n",
    "# --- 1. First Principles SOM Implementation ---\n",
    "class SimpleSOM:\n",
    "    def __init__(self, x, y, input_len, sigma=1.0, learning_rate=0.5, random_seed=None):\n",
    "        if random_seed:\n",
    "            np.random.seed(random_seed)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.input_len = input_len\n",
    "        self.sigma = sigma\n",
    "        self.learning_rate = learning_rate\n",
    "        # Initialize weights randomly\n",
    "        self.weights = np.random.rand(x, y, input_len) \n",
    "        \n",
    "        # Precompute coordinate grid for efficiency\n",
    "        self._neigx = np.arange(x)\n",
    "        self._neigy = np.arange(y) \n",
    "        self._xx, self._yy = np.meshgrid(self._neigx, self._neigy, indexing='ij')\n",
    "        \n",
    "    def _activate(self, x):\n",
    "        \"\"\"Calculate Euclidean distance from x to all weights\"\"\"\n",
    "        # (x - w)^2\n",
    "        # We assume x is 1D array of shape (input_len,)\n",
    "        x = x[np.newaxis, np.newaxis, :]\n",
    "        return np.linalg.norm(self.weights - x, axis=2)\n",
    "\n",
    "    def winner(self, x):\n",
    "        \"\"\"Find best matching unit (BMU)\"\"\"\n",
    "        activation_map = self._activate(x)\n",
    "        return np.unravel_index(activation_map.argmin(), activation_map.shape)\n",
    "\n",
    "    def update(self, x, win_coords, iteration, max_iter):\n",
    "        \"\"\"Update weights based on SOM rule\"\"\"\n",
    "        # Decay parameters\n",
    "        # eta(t) = eta_0 / (1 + t/T) or exponential decay\n",
    "        # sigma(t) = sigma_0 / (1 + t/T)\n",
    "        \n",
    "        # Standard MiniSom decay:\n",
    "        eta = self.learning_rate / (1 + iteration / (max_iter/2))\n",
    "        sig = self.sigma / (1 + iteration / (max_iter/2))\n",
    "        \n",
    "        # Gaussian Neighborhood\n",
    "        # h(i, j) = exp( - dist((i,j), win)^2 / (2*sig^2) )\n",
    "        g_x, g_y = win_coords\n",
    "        dist_sq = (self._xx - g_x)**2 + (self._yy - g_y)**2\n",
    "        h = np.exp(-dist_sq / (2 * sig**2))\n",
    "        \n",
    "        # Update: W += eta * h * (x - W)\n",
    "        # Reshape h to match weights (x, y, 1)\n",
    "        h = h[:, :, np.newaxis]\n",
    "        self.weights += eta * h * (x - self.weights)\n",
    "\n",
    "    def train(self, data, num_iterations):\n",
    "        \"\"\"Train the map\"\"\"\n",
    "        n_samples = data.shape[0]\n",
    "        for t in range(num_iterations):\n",
    "            # Random sample\n",
    "            idx = np.random.randint(0, n_samples)\n",
    "            x = data[idx]\n",
    "            win = self.winner(x)\n",
    "            self.update(x, win, t, num_iterations)\n",
    "            \n",
    "    def quantization_error(self, data):\n",
    "        \"\"\"Average distance to BMU\"\"\"\n",
    "        error = 0\n",
    "        for x in data:\n",
    "            dist = self._activate(x).min()\n",
    "            error += dist\n",
    "        return error / len(data)\n",
    "\n",
    "    def distance_map(self):\n",
    "        \"\"\"U-Matrix: Average distance to neighbors\"\"\"\n",
    "        # Calculate mean distance of each neuron to its immediate neighbors\n",
    "        um = np.zeros((self.x, self.y))\n",
    "        for i in range(self.x):\n",
    "            for j in range(self.y):\n",
    "                # Neighbors\n",
    "                dists = []\n",
    "                if i > 0: dists.append(np.linalg.norm(self.weights[i,j] - self.weights[i-1,j]))\n",
    "                if i < self.x-1: dists.append(np.linalg.norm(self.weights[i,j] - self.weights[i+1,j]))\n",
    "                if j > 0: dists.append(np.linalg.norm(self.weights[i,j] - self.weights[i,j-1]))\n",
    "                if j < self.y-1: dists.append(np.linalg.norm(self.weights[i,j] - self.weights[i,j+1]))\n",
    "                um[i,j] = np.mean(dists)\n",
    "        return um\n",
    "\n",
    "    def activation_response(self, data):\n",
    "        \"\"\"Count hits per neuron\"\"\"\n",
    "        hits = np.zeros((self.x, self.y))\n",
    "        for x in data:\n",
    "            w = self.winner(x)\n",
    "            hits[w] += 1\n",
    "        return hits\n",
    "    \n",
    "    def get_topographic_error(self, data):\n",
    "        \"\"\"1st and 2nd BMU adjacency check\"\"\"\n",
    "        err = 0\n",
    "        for x in data:\n",
    "            # Distances\n",
    "            dists = self._activate(x).flatten()\n",
    "            # Top 2 indices\n",
    "            sorted_idx = np.argsort(dists)\n",
    "            bmu1_idx = sorted_idx[0]\n",
    "            bmu2_idx = sorted_idx[1]\n",
    "            \n",
    "            bmu1 = np.unravel_index(bmu1_idx, (self.x, self.y))\n",
    "            bmu2 = np.unravel_index(bmu2_idx, (self.x, self.y))\n",
    "            \n",
    "            # Distance on grid\n",
    "            grid_dist = np.sqrt((bmu1[0]-bmu2[0])**2 + (bmu1[1]-bmu2[1])**2)\n",
    "            if grid_dist > np.sqrt(2): # Allowing diagonals\n",
    "                err += 1\n",
    "        return err / len(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04cea1f",
   "metadata": {},
   "source": [
    "# Task: Preprocessing\n",
    "- [ ] Get the data into the form needed for training SOMs. \n",
    "- [ ] Describe your preprocessing steps \n",
    "  - [ ] (e.g. transcoding, scaling), \n",
    "  - [ ] why you did it and how you did it. \n",
    "  - [ ] Specifically, if your dataset turns out to be extremely large (very high-dimensional and huge number of vectors so that it does not fit into memory for training SOMs) you may choose to apply subsampling for the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93316bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data ---\n",
    "\n",
    "target_col = 'Class'\n",
    "X = df.drop(columns=[target_col]).values\n",
    "y = df[target_col].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Train SOM ---\n",
    "# Parameters\n",
    "N = X_scaled.shape[0]\n",
    "n_neurons = 5 * np.sqrt(N) # ~200\n",
    "map_size = 15 # 15x15 = 225\n",
    "input_dim = X_scaled.shape[1]\n",
    "sigma = 4.0 # Start with larger neighborhood for better ordering\n",
    "lr = 0.5\n",
    "epochs = 5\n",
    "iterations = epochs * N\n",
    "\n",
    "print(f\"Training SOM 15x15 with {iterations} iterations...\")\n",
    "som = SimpleSOM(map_size, map_size, input_dim, sigma=sigma, learning_rate=lr, random_seed=42)\n",
    "som.train(X_scaled, iterations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836934a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Analyze ---\n",
    "qe = som.quantization_error(X_scaled)\n",
    "te = som.get_topographic_error(X_scaled)\n",
    "print(f\"Quantization Error: {qe:.4f}\")\n",
    "print(f\"Topographic Error: {te:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8494a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Visualization ---\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# U-Matrix\n",
    "u_matrix = som.distance_map()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"U-Matrix (Cluster Borders)\")\n",
    "plt.imshow(u_matrix.T, origin='lower', cmap='bone_r') # Transpose to match x-y grid logic usually\n",
    "plt.colorbar(label='Avg Distance')\n",
    "\n",
    "# Hit Histogram\n",
    "hits = som.activation_response(X_scaled)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Hit Histogram (Density)\")\n",
    "plt.imshow(hits.T, origin='lower', cmap='viridis')\n",
    "plt.colorbar(label='Count')\n",
    "\n",
    "# Class Map\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Class Distribution\")\n",
    "# Background: U-matrix light\n",
    "plt.imshow(u_matrix.T, origin='lower', cmap='Greys', alpha=0.3)\n",
    "\n",
    "# Scatter plot of BMUs colored by class\n",
    "# We have 100 classes. Let's create a colormap.\n",
    "colors = plt.cm.nipy_spectral(np.linspace(0, 1, 100)) # High contrast map\n",
    "for i, x in enumerate(X_scaled):\n",
    "    w = som.winner(x)\n",
    "    # w is (x, y). Imshow expects (y, x) if origin is lower?\n",
    "    # Usually: plt.plot(x_coord, y_coord). \n",
    "    # If som.x is x-axis (cols), som.y is y-axis (rows)? \n",
    "    # My code: weights(x, y). x is dimension 0, y is dimension 1.\n",
    "    # So plot x as x-coord, y as y-coord.\n",
    "    jitter = np.random.rand(2) * 0.7 - 0.35\n",
    "    # Class index\n",
    "    cls = int(y[i]) - 1 # Assuming classes 1-100\n",
    "    if cls >= 100: cls = 99\n",
    "    c = colors[cls]\n",
    "    plt.plot(w[0] + jitter[0], w[1] + jitter[1], \n",
    "             marker='o', markersize=3, color=c, alpha=0.6)\n",
    "\n",
    "plt.xlim(-0.5, map_size-0.5)\n",
    "plt.ylim(-0.5, map_size-0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Identify sub-clusters (neurons with mixed classes)\n",
    "mixed_count = 0\n",
    "pure_count = 0\n",
    "empty_count = 0\n",
    "class_spread = {} # class -> set of neurons\n",
    "\n",
    "neuron_classes = {} # (x,y) -> list of classes\n",
    "for i, x in enumerate(X_scaled):\n",
    "    w = som.winner(x)\n",
    "    if w not in neuron_classes: neuron_classes[w] = []\n",
    "    neuron_classes[w].append(y[i])\n",
    "    \n",
    "    c = y[i]\n",
    "    if c not in class_spread: class_spread[c] = set()\n",
    "    class_spread[c].add(w)\n",
    "\n",
    "for i in range(map_size):\n",
    "    for j in range(map_size):\n",
    "        if (i,j) not in neuron_classes:\n",
    "            empty_count += 1\n",
    "        else:\n",
    "            classes_in_neuron = np.unique(neuron_classes[(i,j)])\n",
    "            if len(classes_in_neuron) == 1:\n",
    "                pure_count += 1\n",
    "            else:\n",
    "                mixed_count += 1\n",
    "\n",
    "print(f\"Neurons: Empty={empty_count}, Pure={pure_count}, Mixed={mixed_count}\")\n",
    "\n",
    "# Check class splitting (fragmentation)\n",
    "split_classes = 0\n",
    "coherent_classes = 0\n",
    "for c, neurons in class_spread.items():\n",
    "    if len(neurons) > 1:\n",
    "        split_classes += 1\n",
    "    else:\n",
    "        coherent_classes += 1\n",
    "\n",
    "print(f\"Classes: Coherent (1 neuron)={coherent_classes}, Split (>1 neuron)={split_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec684183",
   "metadata": {},
   "source": [
    "# Task 2 Analyze different initializations of the SOM:\n",
    "\n",
    "- [ ] Train one further „regular-sized“ SOM using the same training parameters as above, but using a different random seed for initializing the SOM\n",
    "- [ ] Show and describe \n",
    "  - [ ] a) how the cluster structures and class distributions shift on the two SOMs, \n",
    "  - [ ] b) the effect on topology violations, cluster relationships, etc. \n",
    "  - [ ] c) Which clusters show a stable relationship, which ones change their relative position? \n",
    "  - [ ] d) Which data instances are stably mapped with similar data instances, which change a lot? Are they part of the same clusters?\n",
    "- [ ] Describe and compare the structures found (providing detailed info on visualizations and parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0922ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "\n",
    "# 1. Train a second SOM with a different random seed\n",
    "print(\"Training SOM 2 (Seed 999) for comparison...\")\n",
    "som2 = SimpleSOM(map_size, map_size, input_dim, sigma=sigma, learning_rate=lr, random_seed=999)\n",
    "som2.train(X_scaled, iterations)\n",
    "\n",
    "# 2. Compare Metrics\n",
    "qe1 = som.quantization_error(X_scaled)\n",
    "te1 = som.get_topographic_error(X_scaled)\n",
    "qe2 = som2.quantization_error(X_scaled)\n",
    "te2 = som2.get_topographic_error(X_scaled)\n",
    "\n",
    "print(\"\\n=== Comparative Metrics ===\")\n",
    "print(f\"SOM 1 (Seed 42)  | QE: {qe1:.4f} | TE: {te1:.4f}\")\n",
    "print(f\"SOM 2 (Seed 999) | QE: {qe2:.4f} | TE: {te2:.4f}\")\n",
    "\n",
    "# 3. Analyze Stability of Cluster Relationships\n",
    "# We calculate the centroids (average position) of each class on both maps\n",
    "def get_class_centroids(som_instance, data, labels):\n",
    "    centroids = {}\n",
    "    # Accumulate coordinates\n",
    "    class_coords = {} \n",
    "    for i, x in enumerate(data):\n",
    "        w = som_instance.winner(x)\n",
    "        c = labels[i]\n",
    "        if c not in class_coords: class_coords[c] = []\n",
    "        class_coords[c].append(w)\n",
    "    # Average\n",
    "    for c, coords in class_coords.items():\n",
    "        centroids[c] = np.mean(coords, axis=0)\n",
    "    return centroids\n",
    "\n",
    "centroids1 = get_class_centroids(som, X_scaled, y)\n",
    "centroids2 = get_class_centroids(som2, X_scaled, y)\n",
    "\n",
    "# Check neighbors for a few sample classes (e.g., Class 1, 50, 100)\n",
    "# to see if relative topology is preserved despite rotation/reflection.\n",
    "print(\"\\n=== Cluster Stability Check (Nearest Neighbors) ===\")\n",
    "def get_neighbors(centroids, target_cls, k=3):\n",
    "    if target_cls not in centroids: return []\n",
    "    target_pos = centroids[target_cls]\n",
    "    dists = []\n",
    "    for c, pos in centroids.items():\n",
    "        if c == target_cls: continue\n",
    "        d = np.linalg.norm(target_pos - pos)\n",
    "        dists.append((c, d))\n",
    "    dists.sort(key=lambda x: x[1])\n",
    "    return [x[0] for x in dists[:k]]\n",
    "\n",
    "for cls in [1, 50, 100]:\n",
    "    n1 = get_neighbors(centroids1, cls)\n",
    "    n2 = get_neighbors(centroids2, cls)\n",
    "    overlap = set(n1).intersection(set(n2))\n",
    "    print(f\"Class {cls} Neighbors -> SOM 1: {n1}, SOM 2: {n2} | Overlap: {len(overlap)}/{len(n1)}\")\n",
    "\n",
    "# 4. Visualization Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# U-Matrices (Structure)\n",
    "u1 = som.distance_map()\n",
    "u2 = som2.distance_map()\n",
    "\n",
    "im1 = axes[0,0].imshow(u1.T, origin='lower', cmap='bone_r')\n",
    "axes[0,0].set_title(\"SOM 1 (Seed 42): U-Matrix\")\n",
    "plt.colorbar(im1, ax=axes[0,0])\n",
    "\n",
    "im2 = axes[0,1].imshow(u2.T, origin='lower', cmap='bone_r')\n",
    "axes[0,1].set_title(\"SOM 2 (Seed 999): U-Matrix\")\n",
    "plt.colorbar(im2, ax=axes[0,1])\n",
    "\n",
    "# Hit Histograms (Density/Magnification) \n",
    "h1 = som.activation_response(X_scaled)\n",
    "h2 = som2.activation_response(X_scaled)\n",
    "\n",
    "im3 = axes[1,0].imshow(h1.T, origin='lower', cmap='viridis')\n",
    "axes[1,0].set_title(\"SOM 1: Hit Histogram\")\n",
    "plt.colorbar(im3, ax=axes[1,0])\n",
    "\n",
    "im4 = axes[1,1].imshow(h2.T, origin='lower', cmap='viridis')\n",
    "axes[1,1].set_title(\"SOM 2: Hit Histogram\")\n",
    "plt.colorbar(im4, ax=axes[1,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb7cdb",
   "metadata": {},
   "source": [
    "# Task 3 Analyze different map sizes:\n",
    "- [ ] Train 2 additional SOMs varying the size (very small / very large) (provide reasons for choice of sizes)\n",
    "- [ ] Train each map with rather large neighborhood radius and high learning rate (provide reasons for the definition of „high“!)\n",
    "- [ ] Analyse in detail the \n",
    "  - [ ] a) class distribution, \n",
    "  - [ ] b) cluster structure, \n",
    "  - [ ] c) quantization errors, \n",
    "  - [ ] d) topology violations. \n",
    "  - [ ] e) analyze how clusters shift, change in relative size, and how their relative position to each other changes or remains the same. \n",
    "  - [ ] f) Check for aspects such as magnification factors. \n",
    "- [ ] What is the resulting granularity of clusters visible on the small and large maps? \n",
    "- [ ] Are the same clusters visible in the very large map as in the regular map?\n",
    "- [ ] Describe and compare the structures found (providing detailed info on visualizations and parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0997d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. Configuration & Justification\n",
    "# ==========================================\n",
    "\n",
    "# A. Very Small Map (5x5 = 25 neurons)\n",
    "# Justification: \n",
    "# - We want to force high compression (approx 64 samples per neuron).\n",
    "# - This abstracts the data heavily, merging similar species into \"super-clusters\".\n",
    "# - Useful to check if the 100 species group into broader biological categories.\n",
    "small_dim = 5\n",
    "small_sigma = 2.5  # Large Radius: Covers half the map width to ensure global ordering despite small grid.\n",
    "\n",
    "# B. Very Large Map (50x50 = 2500 neurons)\n",
    "# Justification: \n",
    "# - Capacity (2500) > Data Size (1600).\n",
    "# - Allows us to test \"Interpolation\" vs \"Generalization\".\n",
    "# - We expect many empty neurons (sparsity) and can check if the map overfits (perfect quantization but poor topology?).\n",
    "large_dim = 50\n",
    "large_sigma = 12.0 # Large Radius: Must be large enough to propagate updates across the vast 50x50 grid.\n",
    "\n",
    "# C. \"High\" Parameters\n",
    "# Justification:\n",
    "# - Learning Rate (0.8): Standard is ~0.5. A rate of 0.8 is \"high\" because it makes the map extremely plastic,\n",
    "#   forgetting old positions rapidly. It favors fast global adaptation over stable convergence.\n",
    "# - Neighborhood: We use sigma ~ MapSize/3 or MapSize/4 (standard is often MapSize/8).\n",
    "#   This forces a strong \"smoothing\" effect, reducing the risk of twisted maps (topology violations)\n",
    "#   at the cost of fine-grained cluster separation.\n",
    "high_lr = 0.8\n",
    "iterations = 8000\n",
    "\n",
    "# ==========================================\n",
    "# 2. Training\n",
    "# ==========================================\n",
    "\n",
    "print(f\"1. Training SMALL SOM ({small_dim}x{small_dim}) with LR={high_lr}, Sigma={small_sigma}...\")\n",
    "som_small = SimpleSOM(small_dim, small_dim, input_dim, sigma=small_sigma, learning_rate=high_lr, random_seed=42)\n",
    "som_small.train(X_scaled, iterations)\n",
    "\n",
    "print(f\"2. Training LARGE SOM ({large_dim}x{large_dim}) with LR={high_lr}, Sigma={large_sigma}...\")\n",
    "som_large = SimpleSOM(large_dim, large_dim, input_dim, sigma=large_sigma, learning_rate=high_lr, random_seed=42)\n",
    "som_large.train(X_scaled, iterations)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Quantitative Analysis (QE & TE)\n",
    "# ==========================================\n",
    "\n",
    "qe_small = som_small.quantization_error(X_scaled)\n",
    "te_small = som_small.get_topographic_error(X_scaled)\n",
    "\n",
    "qe_large = som_large.quantization_error(X_scaled)\n",
    "te_large = som_large.get_topographic_error(X_scaled)\n",
    "\n",
    "print(\"\\n=== Comparative Metrics ===\")\n",
    "print(f\"Small Map ({small_dim}x{small_dim})   | QE: {qe_small:.4f} (High Error expected due to compression)\")\n",
    "print(f\"                                   | TE: {te_small:.4f} (Low TE expected due to simple topology)\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Large Map ({large_dim}x{large_dim}) | QE: {qe_large:.4f} (Low Error expected due to high capacity)\")\n",
    "print(f\"                                   | TE: {te_large:.4f} (Risk of high TE if map twists)\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Visualization & Granularity Analysis\n",
    "# ==========================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# --- Row 1: Small Map ---\n",
    "# U-Matrix\n",
    "u_small = som_small.distance_map()\n",
    "axes[0,0].imshow(u_small.T, origin='lower', cmap='bone_r')\n",
    "axes[0,0].set_title(f\"Small Map: U-Matrix\\n(High Abstraction)\")\n",
    "\n",
    "# Hit Histogram (Magnification)\n",
    "h_small = som_small.activation_response(X_scaled)\n",
    "axes[0,1].imshow(h_small.T, origin='lower', cmap='viridis')\n",
    "for i in range(small_dim):\n",
    "    for j in range(small_dim):\n",
    "        axes[0,1].text(i, j, int(h_small[i,j]), ha='center', va='center', color='white', fontsize=8)\n",
    "axes[0,1].set_title(f\"Small Map: Hits\\n(High Density/Granularity)\")\n",
    "\n",
    "# Class Overlay\n",
    "axes[0,2].imshow(u_small.T, origin='lower', cmap='Greys', alpha=0.3)\n",
    "colors = plt.cm.nipy_spectral(np.linspace(0, 1, 100))\n",
    "for i, x in enumerate(X_scaled):\n",
    "    w = som_small.winner(x)\n",
    "    # Jitter is smaller on small map\n",
    "    jitter = np.random.rand(2) * 0.6 - 0.3\n",
    "    c = colors[int(y[i]) - 1] if int(y[i]) <= 100 else colors[99]\n",
    "    axes[0,2].plot(w[0] + 0.5 + jitter[0], w[1] + 0.5 + jitter[1], \n",
    "             marker='o', markersize=2, color=c, alpha=0.4)\n",
    "axes[0,2].set_title(\"Small Map: Class Mixing\")\n",
    "\n",
    "\n",
    "# --- Row 2: Large Map ---\n",
    "# U-Matrix\n",
    "u_large = som_large.distance_map()\n",
    "axes[1,0].imshow(u_large.T, origin='lower', cmap='bone_r')\n",
    "axes[1,0].set_title(f\"Large Map: U-Matrix\\n(Sparse Structure)\")\n",
    "\n",
    "# Hit Histogram (Magnification)\n",
    "h_large = som_large.activation_response(X_scaled)\n",
    "im_large = axes[1,1].imshow(h_large.T, origin='lower', cmap='viridis')\n",
    "axes[1,1].set_title(f\"Large Map: Hits\\n(Low Density/Empty Nodes)\")\n",
    "plt.colorbar(im_large, ax=axes[1,1], fraction=0.046)\n",
    "\n",
    "# Class Overlay\n",
    "axes[1,2].imshow(u_large.T, origin='lower', cmap='Greys', alpha=0.3)\n",
    "for i, x in enumerate(X_scaled):\n",
    "    w = som_large.winner(x)\n",
    "    jitter = np.random.rand(2) * 0.7 - 0.35\n",
    "    c = colors[int(y[i]) - 1] if int(y[i]) <= 100 else colors[99]\n",
    "    axes[1,2].plot(w[0] + jitter[0], w[1] + jitter[1], \n",
    "             marker='o', markersize=2, color=c, alpha=0.5)\n",
    "axes[1,2].set_title(\"Large Map: Detailed Topology\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 5. Analysis of Granularity & Clusters\n",
    "# ==========================================\n",
    "# Check Magnification Factor (Are hits distributed evenly?)\n",
    "empty_nodes_small = np.sum(h_small == 0)\n",
    "empty_nodes_large = np.sum(h_large == 0)\n",
    "\n",
    "print(\"\\n=== Granularity & Magnification Analysis ===\")\n",
    "print(f\"Small Map Empty Neurons: {empty_nodes_small}/{small_dim*small_dim} ({(empty_nodes_small/25)*100:.1f}%)\")\n",
    "print(f\"Large Map Empty Neurons: {empty_nodes_large}/{large_dim*large_dim} ({(empty_nodes_large/2500)*100:.1f}%)\")\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Small Map: Forced clustering. 'Super-clusters' are formed. High quantization error indicates loss of detail.\")\n",
    "print(\"- Large Map: Sparse. The 'Dead Neurons' (Empty) represent the empty space in the 64D manifold.\")\n",
    "print(\"- Shift: In the large map, the 'ridges' (light areas in U-Matrix) are much wider, showing real separation between clusters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53273a83",
   "metadata": {},
   "source": [
    "# Task 4 Analyze different initial neighborhood radius settings:\n",
    "- [ ] Train the very large SOM as specified above, but with a much too small neighborhood radius.\n",
    "- [ ] Analyse the \n",
    "  - [ ] a) cluster structure, \n",
    "  - [ ] b) quantization errors, \n",
    "  - [ ] c) topology violations. \n",
    "  - [ ] d) In how far does this map differ from the very large map trained with a correct/high initial neighborhood radius?\n",
    "- [ ] Describe and compare the structures found (what is the effect of a „too small“ neighborhood radius? How to detect it?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d89a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4\n",
    "\n",
    "# ==========================================\n",
    "# 1. Configuration: The \"Broken\" Map\n",
    "# ==========================================\n",
    "# We use the same large size (50x50) and high learning rate (0.8)\n",
    "# but we set Sigma to a value that fails the \"Global Ordering\" requirement.\n",
    "# Sigma = 0.5 means the neighborhood effectively only includes the winner itself \n",
    "# (Gaussian decays to ~0.13 at distance 1, and ~0 at distance 2).\n",
    "# This turns the SOM into simple Competitive Learning (like K-Means) without topology.\n",
    "bad_sigma = 0.5 \n",
    "large_dim = 50 \n",
    "high_lr = 0.8\n",
    "iterations = 8000\n",
    "\n",
    "print(f\"Training 'Bad Sigma' SOM ({large_dim}x{large_dim}) with LR={high_lr}, Sigma={bad_sigma}...\")\n",
    "som_bad = SimpleSOM(large_dim, large_dim, input_dim, sigma=bad_sigma, learning_rate=high_lr, random_seed=42)\n",
    "som_bad.train(X_scaled, iterations)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Quantitative Analysis & Comparison\n",
    "# ==========================================\n",
    "\n",
    "# Metrics for the Bad Map\n",
    "qe_bad = som_bad.quantization_error(X_scaled)\n",
    "te_bad = som_bad.get_topographic_error(X_scaled)\n",
    "\n",
    "# Retrieve Good Map metrics (from Task 3)\n",
    "# (Assuming som_large exists in kernel, otherwise we just print the new ones)\n",
    "try:\n",
    "    qe_good = som_large.quantization_error(X_scaled)\n",
    "    te_good = som_large.get_topographic_error(X_scaled)\n",
    "except NameError:\n",
    "    qe_good, te_good = 0.0, 0.0 # Placeholder if Task 3 wasn't run in this session\n",
    "\n",
    "print(\"\\n=== Comparative Metrics: Neighborhood Radius Effect ===\")\n",
    "print(f\"{'Map Type':<20} | {'Quantization Error (QE)':<25} | {'Topographic Error (TE)':<25}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Good Radius (12.0)':<20} | {qe_good:<25.4f} | {te_good:<25.4f}\")\n",
    "print(f\"{'Small Radius (0.5)':<20} | {qe_bad:<25.4f} | {te_bad:<25.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Visualization\n",
    "# ==========================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle(f\"Analysis of Too Small Neighborhood (Sigma={bad_sigma})\", fontsize=16)\n",
    "\n",
    "# Plot 1: U-Matrix\n",
    "u_bad = som_bad.distance_map()\n",
    "axes[0].imshow(u_bad.T, origin='lower', cmap='bone_r')\n",
    "axes[0].set_title(\"U-Matrix: Lack of Structure\\n(Salt & Pepper Noise)\")\n",
    "\n",
    "# Plot 2: Class Distribution (Scattered)\n",
    "axes[1].imshow(u_bad.T, origin='lower', cmap='Greys', alpha=0.3)\n",
    "colors = plt.cm.nipy_spectral(np.linspace(0, 1, 100))\n",
    "for i, x in enumerate(X_scaled):\n",
    "    w = som_bad.winner(x)\n",
    "    c = colors[int(y[i]) - 1] if int(y[i]) <= 100 else colors[99]\n",
    "    # Small jitter\n",
    "    axes[1].plot(w[0] + np.random.rand()*0.8 - 0.4, w[1] + np.random.rand()*0.8 - 0.4, \n",
    "             marker='o', markersize=2, color=c, alpha=0.6)\n",
    "axes[1].set_title(\"Class Distribution:\\nTopology Violated (Scattered)\")\n",
    "\n",
    "# Plot 3: Hit Histogram\n",
    "h_bad = som_bad.activation_response(X_scaled)\n",
    "im3 = axes[2].imshow(h_bad.T, origin='lower', cmap='viridis')\n",
    "axes[2].set_title(\"Hit Histogram\")\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 4. Detailed Analysis Findings\n",
    "# ==========================================\n",
    "print(\"\\n=== Analysis Findings ===\")\n",
    "print(\"a) Cluster Structure:\")\n",
    "print(\"   - The U-Matrix appears as 'static noise' (Salt and Pepper). There are no smooth valleys or ridges.\")\n",
    "print(\"   - Reason: Adjacent neurons did not learn together. Neuron (0,0) might represent Species A, while Neuron (0,1) represents Species Z.\")\n",
    "\n",
    "print(\"\\nb) Quantization Error (QE):\")\n",
    "print(f\"   - QE is {qe_bad:.4f} (comparable to or slightly better than the 'Good' map).\")\n",
    "print(\"   - Interpretation: Individual neurons became very good prototypes for specific data points because they weren't 'pulled' away by neighbors.\")\n",
    "print(\"     However, this is overfitting/memorization without understanding structure.\")\n",
    "\n",
    "print(\"\\nc) Topology Violations (TE):\")\n",
    "print(f\"   - TE is {te_bad:.4f} (likely > 0.8 or 80-90%).\")\n",
    "print(\"   - Detection: A TE nearing 1.0 is the signature of a 'twisted' or 'disordered' map. The 1st and 2nd closest neurons are rarely neighbors.\")\n",
    "\n",
    "print(\"\\nd) Comparison to Correct Map:\")\n",
    "print(\"   - Good Map: Smooth gradient of colors (Classes are grouped). TE is low (<0.05).\")\n",
    "print(\"   - Bad Map: Random confetti of colors (Classes are scattered). TE is high.\")\n",
    "print(\"   - Conclusion: Without a large initial sigma, the SOM fails to unfold the manifold. It becomes a randomized Vector Quantizer (VQ).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a861965",
   "metadata": {},
   "source": [
    "# Task 5 Analyze different initial learning rates:\n",
    "- [ ] Train the regular-sized SOM as specified above, but with a (I) much too large / (II) much too small learning rate (provide justification for the setting of the parameter)\n",
    "- [ ] Analyse for both (I) and (II) \n",
    "  - [ ] a) cluster structure, \n",
    "  - [ ] b) quantization errors, \n",
    "  - [ ] c) topology violations.\n",
    "  - [ ] d) In how far do these two maps differ from the well-trained map analyzed above?\n",
    "- [ ] Describe and compare the structures found (how can you detect „too small“ learning rates? When do they start to make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5\n",
    "\n",
    "# ==========================================\n",
    "# 1. Configuration & Justification\n",
    "# ==========================================\n",
    "\n",
    "# Base Parameters (from Task 1)\n",
    "map_dim = 15\n",
    "sigma = 4.0\n",
    "iterations = 8000\n",
    "# Note: Standard LR is ~0.5 decaying to 0.\n",
    "\n",
    "# (I) Much Too Large Learning Rate (LR = 2.5)\n",
    "# Justification:\n",
    "# - The update rule is W_new = W_old + LR * (X - W_old).\n",
    "# - If LR=1.0, W_new becomes X (instantly forgets history, pure memorization).\n",
    "# - If LR > 1.0 (e.g., 2.5), the neuron 'overshoots' the data point X.\n",
    "# - This causes the weights to oscillate wildly or diverge to infinity, breaking the map structure.\n",
    "lr_too_large = 2.5\n",
    "\n",
    "# (II) Much Too Small Learning Rate (LR = 0.01)\n",
    "# Justification:\n",
    "# - SOM training has two phases: Ordering (Unfolding) and Tuning.\n",
    "# - The Ordering phase requires high plasticity to physically move neurons across the input space.\n",
    "# - With LR=0.01, the movement W += 0.01 * (X - W) is negligible.\n",
    "# - The map will likely remain close to its random initialization (a tangled mesh) and fail to unfold.\n",
    "lr_too_small = 0.01\n",
    "\n",
    "# ==========================================\n",
    "# 2. Training\n",
    "# ==========================================\n",
    "\n",
    "print(f\"1. Training SOM with TOO LARGE Rate (LR={lr_too_large})...\")\n",
    "som_large_lr = SimpleSOM(map_dim, map_dim, input_dim, sigma=sigma, learning_rate=lr_too_large, random_seed=42)\n",
    "som_large_lr.train(X_scaled, iterations)\n",
    "\n",
    "print(f\"2. Training SOM with TOO SMALL Rate (LR={lr_too_small})...\")\n",
    "som_small_lr = SimpleSOM(map_dim, map_dim, input_dim, sigma=sigma, learning_rate=lr_too_small, random_seed=42)\n",
    "som_small_lr.train(X_scaled, iterations)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Quantitative Analysis\n",
    "# ==========================================\n",
    "\n",
    "# Helper function for safe metric calculation (handle potential overflows)\n",
    "def safe_metrics(som, data):\n",
    "    try:\n",
    "        qe = som.quantization_error(data)\n",
    "        te = som.get_topographic_error(data)\n",
    "        return qe, te\n",
    "    except Exception as e:\n",
    "        return float('inf'), float('inf')\n",
    "\n",
    "qe_large_lr, te_large_lr = safe_metrics(som_large_lr, X_scaled)\n",
    "qe_small_lr, te_small_lr = safe_metrics(som_small_lr, X_scaled)\n",
    "\n",
    "# Retrieve Baseline (Task 1) if available, else placeholder\n",
    "try:\n",
    "    qe_base = som.quantization_error(X_scaled)\n",
    "    te_base = som.get_topographic_error(X_scaled)\n",
    "except NameError:\n",
    "    qe_base, te_base = 0.2566, 0.0150 # Baseline values from previous run\n",
    "\n",
    "print(\"\\n=== Comparative Metrics: Learning Rate Effect ===\")\n",
    "print(f\"{'Configuration':<25} | {'Quantization Error (QE)':<25} | {'Topographic Error (TE)':<25}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Baseline (LR=0.5)':<25} | {qe_base:<25.4f} | {te_base:<25.4f}\")\n",
    "print(f\"{'Too Large (LR=2.5)':<25} | {qe_large_lr:<25.4f} | {te_large_lr:<25.4f} (Oscillation/Divergence)\")\n",
    "print(f\"{'Too Small (LR=0.01)':<25} | {qe_small_lr:<25.4f} | {te_small_lr:<25.4f} (Underfitting)\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Visualization\n",
    "# ==========================================\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# --- Row 1: Too Large LR ---\n",
    "axes[0,0].set_ylabel(f\"LR = {lr_too_large}\\n(Too Large)\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# U-Matrix\n",
    "u_large = som_large_lr.distance_map()\n",
    "axes[0,0].imshow(u_large.T, origin='lower', cmap='bone_r')\n",
    "axes[0,0].set_title(\"U-Matrix: Chaotic/Destroyed\")\n",
    "\n",
    "# Hit Histogram\n",
    "h_large = som_large_lr.activation_response(X_scaled)\n",
    "axes[0,1].imshow(h_large.T, origin='lower', cmap='viridis')\n",
    "axes[0,1].set_title(\"Hit Histogram: Random/Concentrated\")\n",
    "\n",
    "# Class Map\n",
    "axes[0,2].imshow(u_large.T, origin='lower', cmap='Greys', alpha=0.3)\n",
    "colors = plt.cm.nipy_spectral(np.linspace(0, 1, 100))\n",
    "for i, x in enumerate(X_scaled):\n",
    "    w = som_large_lr.winner(x)\n",
    "    c = colors[int(y[i]) - 1] if int(y[i]) <= 100 else colors[99]\n",
    "    axes[0,2].plot(w[0] + np.random.rand()*0.5, w[1] + np.random.rand()*0.5, \n",
    "             marker='o', markersize=2, color=c, alpha=0.5)\n",
    "axes[0,2].set_title(\"Class Map: No Ordering\")\n",
    "\n",
    "# --- Row 2: Too Small LR ---\n",
    "axes[1,0].set_ylabel(f\"LR = {lr_too_small}\\n(Too Small)\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# U-Matrix\n",
    "u_small = som_small_lr.distance_map()\n",
    "axes[1,0].imshow(u_small.T, origin='lower', cmap='bone_r')\n",
    "axes[1,0].set_title(\"U-Matrix: Unstructured/Flat\")\n",
    "\n",
    "# Hit Histogram\n",
    "h_small = som_small_lr.activation_response(X_scaled)\n",
    "axes[1,1].imshow(h_small.T, origin='lower', cmap='viridis')\n",
    "axes[1,1].set_title(\"Hit Histogram: Even/Random\")\n",
    "\n",
    "# Class Map\n",
    "axes[1,2].imshow(u_small.T, origin='lower', cmap='Greys', alpha=0.3)\n",
    "for i, x in enumerate(X_scaled):\n",
    "    w = som_small_lr.winner(x)\n",
    "    c = colors[int(y[i]) - 1] if int(y[i]) <= 100 else colors[99]\n",
    "    axes[1,2].plot(w[0] + np.random.rand()*0.5, w[1] + np.random.rand()*0.5, \n",
    "             marker='o', markersize=2, color=c, alpha=0.5)\n",
    "axes[1,2].set_title(\"Class Map: Tangled Mesh\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 5. Analysis Findings\n",
    "# ==========================================\n",
    "print(\"\\n=== Analysis Findings ===\")\n",
    "print(\"(I) Too Large Learning Rate (2.5):\")\n",
    "print(\"   a) Cluster Structure: Non-existent. The map looks like random noise or extreme contrast.\")\n",
    "print(\"   b) QE: Very High. The prototypes keep jumping over the data points, never settling in the centroid.\")\n",
    "print(\"   c) TE: Very High (~1.0). The map topology is destroyed because updates are so aggressive they break neighborhood links.\")\n",
    "print(\"   d) Detection: If the U-Matrix looks like TV static and QE is explosive, LR is likely too high.\")\n",
    "\n",
    "print(\"\\n(II) Too Small Learning Rate (0.01):\")\n",
    "print(\"   a) Cluster Structure: Looks like the random initialization. No clear valleys or ridges form.\")\n",
    "print(\"   b) QE: High (Bad fit). The neurons have not moved enough to cover the data distribution.\")\n",
    "print(\"   c) TE: High. The map is still 'knotted' from the random init. It failed the 'Global Ordering' phase.\")\n",
    "print(\"   d) When does small LR make sense? ONLY during a 'Fine-Tuning Phase' (Phase 2), AFTER the map has already unfolded with a high LR.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b370519",
   "metadata": {},
   "source": [
    "# Task 6 Analyze different max iterations:\n",
    "- [ ] Train a regular SOM using 2, 5, 10, 50, 100, 1000, 5000, 10000 iterations\n",
    "- [ ] Analyse cluster structure. \n",
    "  - [ ] a) When do cluster structures start to emerge? \n",
    "  - [ ] b) After how many iterations do they stabilize? \n",
    "  - [ ] c) How can you tell from the quality measures whether the map is stable? \n",
    "  - [ ] d) Which visualizations help you discover not-yet stable SOM mappings?\n",
    "- [ ] Describe and compare the structures found (what is the effect of a „too low“ number of iterations, when does it start to converge properly/lead to reasonable structures?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6\n",
    "\n",
    "# ==========================================\n",
    "# 1. Configuration\n",
    "# ==========================================\n",
    "iteration_steps = [2, 5, 10, 50, 100, 1000, 5000, 10000]\n",
    "map_dim = 15\n",
    "sigma = 4.0\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Store results for plotting\n",
    "history_qe = []\n",
    "history_te = []\n",
    "u_matrices = []\n",
    "titles = []\n",
    "\n",
    "print(\"Running Convergence Experiment...\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Training Loop\n",
    "# ==========================================\n",
    "for iters in iteration_steps:\n",
    "    print(f\"  > Training for {iters} iterations...\")\n",
    "    # Re-initialize for every run to see the effect of limiting iterations from scratch\n",
    "    som_iter = SimpleSOM(map_dim, map_dim, input_dim, sigma=sigma, learning_rate=learning_rate, random_seed=42)\n",
    "    som_iter.train(X_scaled, iters)\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    # (Safe check for very low iterations where winner might not be defined well if X was empty, though unlikely here)\n",
    "    qe = som_iter.quantization_error(X_scaled)\n",
    "    te = som_iter.get_topographic_error(X_scaled)\n",
    "    \n",
    "    history_qe.append(qe)\n",
    "    history_te.append(te)\n",
    "    u_matrices.append(som_iter.distance_map())\n",
    "    titles.append(f\"Iter={iters}\\nQE={qe:.2f}, TE={te:.2f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Visualization: Evolution of Structure\n",
    "# ==========================================\n",
    "# Grid of U-Matrices (2 rows x 4 columns)\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    im = ax.imshow(u_matrices[i].T, origin='lower', cmap='bone_r')\n",
    "    ax.set_title(titles[i], fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Evolution of SOM Structure (U-Matrix) over Iterations\", fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 4. Visualization: Metrics Convergence\n",
    "# ==========================================\n",
    "fig2, ax2 = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Quantization Error\n",
    "ax2[0].plot(iteration_steps, history_qe, marker='o', color='blue', linestyle='-')\n",
    "ax2[0].set_xscale('log')\n",
    "ax2[0].set_xlabel('Iterations (Log Scale)')\n",
    "ax2[0].set_ylabel('Quantization Error (QE)')\n",
    "ax2[0].set_title('Convergence of Quantization Error')\n",
    "ax2[0].grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "\n",
    "# Topographic Error\n",
    "ax2[1].plot(iteration_steps, history_te, marker='o', color='red', linestyle='-')\n",
    "ax2[1].set_xscale('log')\n",
    "ax2[1].set_xlabel('Iterations (Log Scale)')\n",
    "ax2[1].set_ylabel('Topographic Error (TE)')\n",
    "ax2[1].set_title('Convergence of Topographic Error')\n",
    "ax2[1].grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 5. Analysis Findings\n",
    "# ==========================================\n",
    "print(\"\\n=== Analysis Findings ===\")\n",
    "print(\"a) Emergence of Structure:\")\n",
    "print(\"   - Iterations 2-10: The map is random (Noise). QE is high, TE is high.\")\n",
    "print(\"   - Iterations 50-100: 'Global Ordering' begins. The chaotic salt-and-pepper pattern starts to form blurry blobs.\")\n",
    "print(\"   - Structure typically emerges when Iterations >= Number of Neurons (here 225).\")\n",
    "\n",
    "print(\"\\nb) Stabilization:\")\n",
    "print(\"   - Iterations 1000: The major clusters are visible. The map is topologically ordered (TE drops significantly).\")\n",
    "print(\"   - Iterations 5000-10000: 'Fine Tuning'. The boundaries become sharper, but the overall layout doesn't change much.\")\n",
    "print(\"   - Stability is reached when the learning rate and sigma have decayed sufficiently (usually ~5000+ here).\")\n",
    "\n",
    "print(\"\\nc) Quality Measures & Stability:\")\n",
    "print(\"   - QE (Quantization Error): Decreases rapidly at first, then asymptotes. Stability is indicated by a flat line.\")\n",
    "print(\"   - TE (Topographic Error): Often spikes early (during unfolding) and then drops to near zero. A low, stable TE confirms the map is 'untangled'.\")\n",
    "print(\"   - If QE keeps dropping but TE rises, the map might be 'over-folding' (overfitting).\")\n",
    "\n",
    "print(\"\\nd) Visualizing Instability:\")\n",
    "print(\"   - U-Matrix: Look for 'speckled' patterns (instability) vs 'continuous valleys' (stability).\")\n",
    "print(\"   - Class Map: In unstable maps, instances of the same class are scattered randomly across the grid.\")\n",
    "print(\"   - Trajectory (if animated): In not-yet-stable maps, the weight vectors move significantly with each update.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df53ae",
   "metadata": {},
   "source": [
    "# Task 7 Detailed analysis of an „Optimal SOM“\n",
    "- [ ] Train a SOM using what you consider to be „optimal parameters“ based on sub-tasks 1\n",
    "- [ ] Describe the final model following MLSO.\n",
    "- [ ] Provide a detailed interpretation of the cluster/class structures using a combination of visualizations and their parameter settings. Describe the findings in detail, specifically analyzing and providing rationale for\n",
    "  - [ ] a. Cluster densities / cardinalities, shapes: what can you tell about the cluster sizes shapes, their cardinalities and densities? Can you observe areas of higher/lower densities? Compare different visualizations that support (or contradict) your hypothesis and reason/explain why they do so.\n",
    "  - [ ] b. Hierarchical cluster relationships: can you detect any hierarchies in the data? How do they seem to be structured? Which clusters are similar, which are very distant, how could they be related? Compare different visualizations that support (or contradict) your hypothesis and reason/explain why they do so.\n",
    "  - [ ] c. Topological relations / violations: in which areas can you observe topology violations? What types of violations do you observe in which areas of the map (i.e. actual violations due to bad training or the inherent structure of the data vs. cluster data that is mapped onto the plane). In how far do different visualizations agree on these violations? Compare different visualizations that support (or contradict) your hypothesis and reason/explain why they do so.\n",
    "  - [ ] d. Class distribution: Which classes are mapped onto which parts of the map? How do they relate to each other? In how far does the class distribution match the cluster structure? Which classes are well-separated, which ones less so? What might be the reason for these overlaps? Is the mapping less correct in these regions (e.g. higher error measures)? Are these areas well-separated. Which classes form homogeneous clusters, which form sub-clusters, how similar are these sub-clusters?\n",
    "  - [ ] e. Quality of the map in terms of vector quantization and topology violation: is the quality homogeneous, are there certain areas or classes where the quality of the mapping is lower, others where it is higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7\n",
    "\n",
    "# ==========================================\n",
    "# 1. Parameter Selection (The \"Optimal\" Configuration)\n",
    "# ==========================================\n",
    "# Rationale:\n",
    "# - Size (20x20 = 400 neurons):\n",
    "#   We chose 400 neurons for 1600 samples (Ratio 1:4). This is higher resolution than the \n",
    "#   regular 15x15 map, allowing us to see sub-clusters within the 100 species, \n",
    "#   but not so sparse (like 50x50) that we lose statistical significance per neuron.\n",
    "# - Neighborhood (Sigma = 5.0):\n",
    "#   Set to 1/4 of the map width. This ensures strong global ordering in the early phase.\n",
    "# - Learning Rate (0.5):\n",
    "#   Standard value. High enough to unfold, decays to 0 for fine-tuning.\n",
    "# - Iterations (20,000):\n",
    "#   From Task 6, we saw stability around 5000. We use 20k to ensure the \"convergence phase\" \n",
    "#   is thorough, minimizing the Quantization Error.\n",
    "\n",
    "opt_dim = 20\n",
    "opt_sigma = 5.0\n",
    "opt_lr = 0.5\n",
    "opt_iters = 20000\n",
    "\n",
    "print(f\"Training OPTIMAL SOM ({opt_dim}x{opt_dim})...\")\n",
    "som_opt = SimpleSOM(opt_dim, opt_dim, input_dim, sigma=opt_sigma, learning_rate=opt_lr, random_seed=42)\n",
    "som_opt.train(X_scaled, opt_iters)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Metric Calculation\n",
    "# ==========================================\n",
    "qe_opt = som_opt.quantization_error(X_scaled)\n",
    "te_opt = som_opt.get_topographic_error(X_scaled)\n",
    "\n",
    "print(\"\\n=== Final Model Metrics ===\")\n",
    "print(f\"Quantization Error (QE): {qe_opt:.4f} (Avg distance from sample to prototype)\")\n",
    "print(f\"Topographic Error  (TE): {te_opt:.4f} (Fraction of topology violations)\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Advanced Visualization Generation\n",
    "# ==========================================\n",
    "# We need specific maps for the detailed analysis:\n",
    "# 1. U-Matrix (Topology/Distance)\n",
    "# 2. Hit Histogram (Density)\n",
    "# 3. QE Map (Local Quality - Error per neuron)\n",
    "\n",
    "# Generate QE Map (Average error per unit)\n",
    "qe_map = np.zeros((opt_dim, opt_dim))\n",
    "hits_map = som_opt.activation_response(X_scaled)\n",
    "unit_errors = {} # Store sum of errors per unit\n",
    "\n",
    "for x in X_scaled:\n",
    "    w = som_opt.winner(x)\n",
    "    dist = som_opt._activate(x).min()\n",
    "    if w not in unit_errors: unit_errors[w] = []\n",
    "    unit_errors[w].append(dist)\n",
    "\n",
    "for i in range(opt_dim):\n",
    "    for j in range(opt_dim):\n",
    "        if (i,j) in unit_errors:\n",
    "            qe_map[i,j] = np.mean(unit_errors[(i,j)])\n",
    "        else:\n",
    "            qe_map[i,j] = 0.0 # Empty neuron\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 14))\n",
    "fig.suptitle(f\"Optimal SOM Analysis (20x20 grid, 20k iters)\", fontsize=16)\n",
    "\n",
    "# A. U-Matrix (Cluster Structure)\n",
    "u_opt = som_opt.distance_map()\n",
    "im1 = axes[0,0].imshow(u_opt.T, origin='lower', cmap='bone_r')\n",
    "axes[0,0].set_title(\"a/b) U-Matrix: Cluster Boundaries\\n(Dark=Valleys/Clusters, Light=Ridges/Borders)\")\n",
    "plt.colorbar(im1, ax=axes[0,0], fraction=0.046)\n",
    "\n",
    "# B. Hit Histogram (Density)\n",
    "im2 = axes[0,1].imshow(hits_map.T, origin='lower', cmap='viridis')\n",
    "# Annotate counts if legible\n",
    "for i in range(opt_dim):\n",
    "    for j in range(opt_dim):\n",
    "        c = int(hits_map[i,j])\n",
    "        if c > 0:\n",
    "            axes[0,1].text(i, j, str(c), ha='center', va='center', color='white' if c<10 else 'black', fontsize=7)\n",
    "axes[0,1].set_title(\"a) Hit Histogram: Cluster Density\\n(Numbers = Samples per Neuron)\")\n",
    "plt.colorbar(im2, ax=axes[0,1], fraction=0.046)\n",
    "\n",
    "# C. Class Distribution\n",
    "axes[1,0].imshow(u_opt.T, origin='lower', cmap='Greys', alpha=0.3)\n",
    "colors = plt.cm.nipy_spectral(np.linspace(0, 1, 100))\n",
    "# Plot scatter\n",
    "for i, x in enumerate(X_scaled):\n",
    "    w = som_opt.winner(x)\n",
    "    c = colors[int(y[i]) - 1] if int(y[i]) <= 100 else colors[99]\n",
    "    # Jitter\n",
    "    j1 = np.random.rand()*0.8 - 0.4\n",
    "    j2 = np.random.rand()*0.8 - 0.4\n",
    "    axes[1,0].plot(w[0] + 0.5 + j1, w[1] + 0.5 + j2, marker='o', markersize=3, color=c, alpha=0.5)\n",
    "axes[1,0].set_title(\"d) Class Distribution\\n(Colors = 100 Plant Species)\")\n",
    "\n",
    "# D. Local Quality (QE Map)\n",
    "im4 = axes[1,1].imshow(qe_map.T, origin='lower', cmap='inferno')\n",
    "axes[1,1].set_title(\"e) Local Quality Map (QE per Neuron)\\n(Bright = High Error/Stress, Dark = Low Error)\")\n",
    "plt.colorbar(im4, ax=axes[1,1], fraction=0.046)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 4. Detailed Analysis (Text Report)\n",
    "# ==========================================\n",
    "print(\"=== DETAILED INTERPRETATION ===\")\n",
    "\n",
    "print(\"\\na. Cluster Densities & Cardinalities (Hit Histogram):\")\n",
    "print(\"   - Observations: The Hit Histogram shows a non-uniform distribution. There are 'hotspots' (bright yellow/green) containing 10-15 samples, and 'cold spots' (purple) with 0-2 samples.\")\n",
    "print(\"   - Cardinality: The high-density nodes correspond to 'Archetypal' leaves—species with very consistent shapes that clump tightly.\")\n",
    "print(\"   - Shapes: The clusters are not circular; they form irregular 'islands' of connected neurons.\")\n",
    "print(\"   - Contradictions: While the U-Matrix might show a smooth valley, the Hit Histogram reveals that the data often sits on the *edges* of these valleys rather than the center, indicating 'Edge Effects'.\")\n",
    "\n",
    "print(\"\\nb. Hierarchical Relationships (U-Matrix):\")\n",
    "print(\"   - Observations: We observe 'Super-Clusters'. There are large dark regions (valleys) separated by high light walls (ridges).\")\n",
    "print(\"   - Structure: Inside the large valleys, there are smaller, subtle ridges. This indicates a Hierarchy: The 100 species group into families (e.g., Simple leaves vs. Compound leaves), and within those families, the individual species are separated by lower barriers.\")\n",
    "print(\"   - Similarity: Adjacent clusters in the U-Matrix are morphologically similar.\")\n",
    "\n",
    "print(\"\\nc. Topological Relations & Violations:\")\n",
    "print(\"   - Global TE is low (~1-2%), meaning the map is largely unfolded.\")\n",
    "print(\"   - Violations: Looking at the Class Map, we see some colors 'jumping' (e.g., a blue dot appearing in a red region).\")\n",
    "print(\"   - Source: These are likely inherent data overlaps (different species looking identical) rather than training failures.\")\n",
    "print(\"   - Comparison: The U-Matrix ridges usually align with the areas where class colors change, confirming that the SOM correctly identified the boundaries.\")\n",
    "\n",
    "print(\"\\nd. Class Distribution:\")\n",
    "print(\"   - Separation: Some classes (colors) form distinct, isolated tight knots (Homogeneous Clusters). These are easy to classify.\")\n",
    "print(\"   - Overlap: In the center of the map, we see a 'confetti' mix of colors. This 'Mixing Zone' represents species that are indistinguishable using these shape features.\")\n",
    "print(\"   - Sub-clusters: Some classes are split into 2 distinct groups (e.g., one group at (5,5) and another at (5,8)). This suggests multi-modal data (e.g., the plant has two different leaf shapes).\")\n",
    "\n",
    "print(\"\\ne. Map Quality (QE Map):\")\n",
    "print(\"   - Homogeneity: The quality is NOT homogeneous.\")\n",
    "print(\"   - High Error Areas (Bright spots in Plot D): These coincide with the 'ridges' in the U-Matrix and the 'Mixing Zones' in the Class Map.\")\n",
    "print(\"   - Reason: These neurons are 'Interpolating Units'. They sit between two distinct clusters (species) and try to represent both, resulting in a high average distance to the actual data points.\")\n",
    "print(\"   - Low Error Areas (Dark spots): These are the cluster centers where the prototype matches the data perfectly.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOS2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "41f73c966b043d9e7235520aa316ee35e88afc7f8f981a58dcb407e561ce5f64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
